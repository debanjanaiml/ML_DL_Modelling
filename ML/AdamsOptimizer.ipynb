{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam's Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Math\n",
    "\n",
    "Let us consider a distribution of data which is linearly distributed.\n",
    "\n",
    "Equation of a straight line:\n",
    "\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Equations when using batch gradient descent:\n",
    "\n",
    "The cost function can be represented as:\n",
    "\n",
    "$J(\\theta_0,\\theta_1) = {1 \\over 2m} \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)^2$\n",
    "\n",
    "We need to minimize our cost function to the global minima.\n",
    "\n",
    "The global minima can be effectively reached using various techniques.\n",
    "\n",
    "In order to do that, we need to the find the gradients of the features.\\\n",
    "Gradients will be:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m} \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m} \\sum\\limits_{i=1}^m ((h_\\theta(x_i)-y_i) \\cdot x_i)$\n",
    "\n",
    "\n",
    "Equations when using stochastic gradient descent:\n",
    "\n",
    "The cost function to be minimized:\n",
    "\n",
    "$J(\\theta_0,\\theta_1) = {1 \\over 2} (h_\\theta(x_i)-y_i)^2$\n",
    "\n",
    "Gradients will be:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = h_\\theta(x_i)-y_i$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = (h_\\theta(x_i)-y_i) \\cdot x_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the equation of a straight line. where theta_0 is the intercept and theta_1 is the slope.\n",
    "\n",
    "h = lambda theta_0, theta_1, x: theta_0 + theta_1*x\n",
    "\n",
    "# the cost function (for the whole batch. for comparison later)\n",
    "def J(x, y, theta_0, theta_1):\n",
    "    m = len(x)\n",
    "    returnValue = 0\n",
    "    for i in range(m):\n",
    "        returnValue += (h(theta_0, theta_1, x[i]) - y[i])**2\n",
    "    returnValue = returnValue/(2*m)\n",
    "    return returnValue\n",
    "\n",
    "# finding the gradient per each training example\n",
    "def grad_J(x, y, theta_0, theta_1):\n",
    "    returnValue = np.array([0., 0.])\n",
    "    returnValue[0] += (h(theta_0, theta_1, x) - y)\n",
    "    returnValue[1] += (h(theta_0, theta_1, x) - y)*x\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam's Optimizer\n",
    "\n",
    "We can define our Adam's Optimizer which we can use with various classification as well as Neural Networks.\\\n",
    "It is made with reference to https://arxiv.org/pdf/1412.6980v8.pdf\n",
    "\n",
    "We can use or tune the default values of:\\\n",
    "The hyper-parameters present here are:\\\n",
    "$\\alpha$         (learning rate)    = 0.001 (say).\\\n",
    "$\\beta_1$ (exp decay rate)   = 0.9.\\\n",
    "$\\beta_2$ (curr w,b)         = 0.999.\\\n",
    "$\\epsilon$        (tolerance)      = 1e-8\n",
    "\n",
    "$\\theta$  (Initial Parameter Vector)\\\n",
    "$m_0$ = 0  (Initialize 1st Moment Vector)\\\n",
    "$v_0$ = 0  (Initialize 2nd Moment Vector)\\\n",
    "t = 0  (Initialize timestep)\n",
    "\n",
    "If $\\theta$ is not converged, perform:\n",
    "\n",
    "$t = t + 1$\n",
    "\n",
    "Get Gradients of stochastic objective at time t\\\n",
    "$ g_t = div.f_t(\\theta_t-1) $\n",
    "\n",
    "Update biased first moment estimate:\\\n",
    "$ m_t = \\beta_1.m_t + (1-\\beta_1).g_t $\n",
    "\n",
    "Update biased second raw moment estimate:\\\n",
    "$ v_t = \\beta_1.v_t + (1-\\beta_1).g_t^2$\n",
    "\n",
    "Here we also do bias and weight correction as well\n",
    "\n",
    "Compute bias correction of first moment\n",
    "\n",
    "$ mhat_t = {m_t\\over(1 - \\beta_1^t)}$\n",
    "\n",
    "Compute bias correction of second raw moment\n",
    "\n",
    "$ vhat_t = {v_t\\over(1 - \\beta_2^t)}$\n",
    "\n",
    "Update parameters:\n",
    "\n",
    "$ \\theta_t = \\theta_t - \\alpha.{mhat_t\\over(sqrt(vhat_t) + \\epsilon) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, weights, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "        self.theta = weights\n",
    "        \n",
    "    def pass_back(self, gradient):\n",
    "        self.t = self.t + 1\n",
    "        # Update First Biased Moment Estimate\n",
    "        self.m = self.beta1*self.m + (1 - self.beta1)*gradient\n",
    "        # Update Second Biased Raw Moment Estimate\n",
    "        self.v = self.beta2*self.v + (1 - self.beta2)*(gradient**2)\n",
    "        # Bias Correction\n",
    "        m_hat = self.m/(1 - self.beta1**self.t)\n",
    "        v_hat = self.v/(1 - self.beta2**self.t)\n",
    "        # Update Parameters\n",
    "        self.theta = self.theta - self.alpha*(m_hat/(np.sqrt(v_hat) - self.epsilon))\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "epochs = 15000\n",
    "print_interval = 1000\n",
    "m = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial value of theta and cost function, before gradient descent\n",
    "initial_theta = np.array([0., 0.]) \n",
    "initial_cost = J(x, y, initial_theta[0], initial_theta[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Stochastic Gradient Descent with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "alpha = 0.001 \n",
    "theta = initial_theta\n",
    "\n",
    "# Plot SGD trajectory history\n",
    "sgd_history = []\n",
    "sgd_history.append(dict({'theta': theta, 'cost': initial_cost}))\n",
    "\n",
    "for j in range(epochs):\n",
    "    for i in range(m):\n",
    "        # Calculate Gradient of cost-funtion using grad_J()\n",
    "        gradients = grad_J(x[i], y[i], theta[0], theta[1])\n",
    "        theta -= gradients*alpha\n",
    "    \n",
    "    # We need to plot datapoints only per print interval\n",
    "    if ((j+1)%print_interval == 0 or j==0):\n",
    "        cost = J(x, y, theta[0], theta[1])\n",
    "        print ('After {} epochs, Cost = {}, theta = {}'.format(j+1, cost, theta))\n",
    "        sgd_history.append(dict({'theta': theta, 'cost': cost}))\n",
    "        \n",
    "print('\\nFinal theta = {}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent using Adams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = initial_theta\n",
    "# call AdamOptimizer class with hyperparameters\n",
    "adam_optimizer = AdamOptimizer(theta, alpha=0.001)\n",
    "\n",
    "# Plot SGD with Adams trajectory history\n",
    "adam_history = []\n",
    "adam_history.append(dict({'theta': theta, 'cost': initial_cost}))\n",
    "\n",
    "for j in range(epochs):\n",
    "    for i in range(m):\n",
    "        # Calculate Gradient of cost-funtion same as SGD\n",
    "        gradients = grad_J(x[i], y[i], theta[0], theta[1])\n",
    "        # Fetch the optimized theta value using adam's technique\n",
    "        theta = adam_optimizer.pass_back(gradients)\n",
    "    \n",
    "    # Plot trajectory only per print interval\n",
    "    if ((j+1)%print_interval == 0 or j==0):\n",
    "        cost = J(x, y, theta[0], theta[1])\n",
    "        print ('After {} epochs, Cost = {}, theta = {}'.format(j+1, cost, theta))\n",
    "        adam_history.append(dict({'theta': theta, 'cost': cost}))\n",
    "        \n",
    "print ('\\nFinal theta = {}'.format(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
